{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmugabo/Group-7-Malaria-Diagnosis-CNN-Transfer-Learning/blob/main/Jallah_VGG16_Model_Malaria_Diagnosis_CNN_Group7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_hxzrVpanrY"
      },
      "source": [
        "# Deep Learning for Malaria Diagnosis\n",
        "This notebook is inspired by works of (Sivaramakrishnan Rajaraman  et al., 2018) and (Jason Brownlee, 2019). Acknowledge to NIH and Bangalor Hospital who make available this malaria dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DyHvXlda9rH"
      },
      "source": [
        "Malaria is an infectuous disease caused by parasites that are transmitted to people through the bites of infected female Anopheles mosquitoes.\n",
        "\n",
        "The Malaria burden with some key figures:\n",
        "<font color='red'>\n",
        "* More than 219 million cases\n",
        "* Over 430 000 deaths in 2017 (Mostly: children & pregnants)\n",
        "* 80% in 15 countries of Africa & India\n",
        "  </font>\n",
        "\n",
        "![MalariaBurd](https://github.com/habiboulaye/ai-labs/blob/master/malaria-diagnosis/doc-images/MalariaBurden.png?raw=1)\n",
        "\n",
        "The malaria diagnosis is performed using blood test:\n",
        "* Collect patient blood smear\n",
        "* Microscopic visualisation of the parasit\n",
        "\n",
        "![MalariaDiag](https://github.com/habiboulaye/ai-labs/blob/master/malaria-diagnosis/doc-images/MalariaDiag.png?raw=1)\n",
        "  \n",
        "Main issues related to traditional diagnosis:\n",
        "<font color='#ed7d31'>\n",
        "* resource-constrained regions\n",
        "* time needed and delays\n",
        "* diagnosis accuracy and cost\n",
        "</font>\n",
        "\n",
        "The objective of this notebook is to apply modern deep learning techniques to perform medical image analysis for malaria diagnosis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5qBTeqkrJ88"
      },
      "source": [
        "*This notebook is inspired by works of (Sivaramakrishnan Rajaraman  et al., 2018), (Adrian Rosebrock, 2018) and (Jason Brownlee, 2019)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K5rb4bmdMRf"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jxaLbRUnYWTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be6c1a8-3955-491f-a56d-a3e2505c52d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "ls: cannot access '/content/drive/My Drive/Colab Notebooks/10xDS/Projects/malaria-diagnosis/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#Mount the local drive project_forder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/10xDS/Projects/malaria-diagnosis/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oIfORUX7ccHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "44e9c8ca-083f-41ea-c676-3dbb8ebc6956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Use GPU: Please check if the outpout is '/device:GPU:0'\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.test.gpu_device_name()\n",
        "#from tensorflow.python.client import device_lib\n",
        "#device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp1o6Cd7dV6Z"
      },
      "source": [
        "## Populating namespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_4Ph8e1uojEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5780ca6-4425-49ca-de74-1a1ab0233240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os, random, json, pathlib, itertools\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, roc_curve, auc)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "SEED=42\n",
        "tf.random.set_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
        "print(\"TensorFlow:\",tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SOvmLtdRgSIb"
      },
      "outputs": [],
      "source": [
        "# Define the useful paths for data accessibility\n",
        "ai_project = '.' #\"/content/drive/My Drive/Colab Notebooks/ai-labs/malaria-diagnosis\"\n",
        "cell_images_dir = os.path.join(ai_project,'cell_images')\n",
        "training_path = os.path.join(ai_project,'train')\n",
        "testing_path = os.path.join(ai_project,'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11DKlCJcj31w"
      },
      "source": [
        "## Prepare DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "midATIuUq7H7"
      },
      "source": [
        "### *Download* DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eCT2ogQdeHPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3dedab-eb32-466b-d59e-af1f6ff615d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-05 16:36:42--  https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\n",
            "Resolving data.lhncbc.nlm.nih.gov (data.lhncbc.nlm.nih.gov)... 3.165.102.96, 3.165.102.59, 3.165.102.109, ...\n",
            "Connecting to data.lhncbc.nlm.nih.gov (data.lhncbc.nlm.nih.gov)|3.165.102.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353452851 (337M) [application/zip]\n",
            "Saving to: ‘cell_images.zip’\n",
            "\n",
            "cell_images.zip     100%[===================>] 337.08M   393MB/s    in 0.9s    \n",
            "\n",
            "2025-10-05 16:36:43 (393 MB/s) - ‘cell_images.zip’ saved [353452851/353452851]\n",
            "\n",
            "cell_images\t drive\t\tsample_data\n",
            "cell_images.zip  malaria_split\tvgg16_experiments\n"
          ]
        }
      ],
      "source": [
        "# Download the data in the allocated google cloud-server. If already down, turn downloadData=False\n",
        "downloadData = True\n",
        "if downloadData == True:\n",
        "  indrive = False\n",
        "  if indrive == True:\n",
        "    !wget https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip -P \"/content/drive/My Drive/Colab Notebooks/ai-labs/malaria-diagnosis\"\n",
        "    !unzip \"/content/drive/My Drive/Colab Notebooks/ai-labs/malaria-diagnosis/cell_images.zip\" -d \"/content/drive/My Drive/Colab Notebooks/ai-labs/malaria-diagnosis/\"\n",
        "    !ls \"/content/drive/My Drive/Colab Notebooks/ai-labs/malaria-diagnosis\"\n",
        "  else: #incloud google server\n",
        "    !rm -rf cell_images.*\n",
        "    !wget https://data.lhncbc.nlm.nih.gov/public/Malaria/cell_images.zip\n",
        "    !unzip cell_images.zip >/dev/null 2>&1\n",
        "    !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LUJGE9U2vW"
      },
      "source": [
        "## Baseline CNN Model\n",
        "Define a basic ConvNet defined with ConvLayer: Conv2D => MaxPooling2D followed by Flatten => Dense => Dense(output)\n",
        "\n",
        "![ConvNet](https://github.com/habiboulaye/ai-labs/blob/master/malaria-diagnosis/doc-images/ConvNet.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Model: VGG16"
      ],
      "metadata": {
        "id": "mpwyzY59IWOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "1.   Implement and fine-tune VGG16 for malaria cell classification.\n",
        "2.   Conduct seven experiments systematically varying augmentation, fine-tuning depth, optimizer, and regularization.\n",
        "3.   Evaluate models rigorously using accuracy, precision, recall, F1, and\n",
        "2.   visualize learning curves, confusion matrices, and ROC/AUC.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XoIQoCj3KelW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16’s simple, uniform 3×3 conv stacks form a strong baseline and a common medical-imaging transfer model. Despite being parameter-heavy, it transfers well when paired with global pooling, dropout, and careful learning-rate schedules."
      ],
      "metadata": {
        "id": "TWLvWWUeIqX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, itertools, pathlib, shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, roc_curve, auc\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "RAW_ROOT   = \"/content/cell_images\"\n",
        "SPLIT_ROOT = \"/content/cell_images_split\"\n",
        "OUTPUT_DIR = \"/content/vgg16_experiments\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u04eDog-RRjH",
        "outputId": "df861637-2594-42ee-8e52-5091e7645c62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7095565c"
      },
      "source": [
        "Train a VGG16 model, run at least seven experiments with different configurations, document the choices made, evaluate the model rigorously using accuracy, precision, recall, and F1-score presented in tables, and include visual evidence of evaluation such as learning curves, confusion matrices, and ROC/AUC curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6516bf81"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Split the dataset into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT5Es6fVXYJc",
        "outputId": "e5697437-7336-4700-82fb-8c97b819f49e"
      },
      "source": [
        "# Remove the split directory if it exists\n",
        "if os.path.exists(SPLIT_ROOT):\n",
        "    shutil.rmtree(SPLIT_ROOT)\n",
        "\n",
        "# Create the directory structure for the split dataset\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    for class_name in ['Parasitized', 'Uninfected']:\n",
        "        os.makedirs(os.path.join(SPLIT_ROOT, split_name, class_name), exist_ok=True)\n",
        "\n",
        "# Get the list of image files and shuffle them\n",
        "parasitized_files = list(pathlib.Path(RAW_ROOT, 'Parasitized').glob('*.png'))\n",
        "uninfected_files = list(pathlib.Path(RAW_ROOT, 'Uninfected').glob('*.png'))\n",
        "\n",
        "random.shuffle(parasitized_files)\n",
        "random.shuffle(uninfected_files)\n",
        "\n",
        "# Determine the number of files for each split\n",
        "total_parasitized = len(parasitized_files)\n",
        "total_uninfected = len(uninfected_files)\n",
        "\n",
        "train_split = 0.8\n",
        "val_split = 0.1\n",
        "test_split = 0.1\n",
        "\n",
        "train_parasitized_count = int(total_parasitized * train_split)\n",
        "val_parasitized_count = int(total_parasitized * val_split)\n",
        "test_parasitized_count = total_parasitized - train_parasitized_count - val_parasitized_count\n",
        "\n",
        "train_uninfected_count = int(total_uninfected * train_split)\n",
        "val_uninfected_count = int(total_uninfected * val_split)\n",
        "test_uninfected_count = total_uninfected - train_uninfected_count - val_uninfected_count\n",
        "\n",
        "# Copy the files to the respective directories\n",
        "def copy_files(file_list, destination_dir):\n",
        "    for file_path in file_list:\n",
        "        shutil.copy(file_path, destination_dir)\n",
        "\n",
        "# Copy Parasitized files\n",
        "copy_files(parasitized_files[:train_parasitized_count], os.path.join(SPLIT_ROOT, 'train', 'Parasitized'))\n",
        "copy_files(parasitized_files[train_parasitized_count:train_parasitized_count + val_parasitized_count], os.path.join(SPLIT_ROOT, 'val', 'Parasitized'))\n",
        "copy_files(parasitized_files[train_parasitized_count + val_parasitized_count:], os.path.join(SPLIT_ROOT, 'test', 'Parasitized'))\n",
        "\n",
        "# Copy Uninfected files\n",
        "copy_files(uninfected_files[:train_uninfected_count], os.path.join(SPLIT_ROOT, 'train', 'Uninfected'))\n",
        "copy_files(uninfected_files[train_uninfected_count:train_uninfected_count + val_uninfected_count], os.path.join(SPLIT_ROOT, 'val', 'Uninfected'))\n",
        "copy_files(uninfected_files[train_uninfected_count + val_uninfected_count:], os.path.join(SPLIT_ROOT, 'test', 'Uninfected'))\n",
        "\n",
        "print(\"Dataset split and copied successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split and copied successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a516d86d"
      },
      "source": [
        "## Define model building function\n",
        "\n",
        "Create a function to build and compile the VGG16 model with customizable layers, optimizer, and regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "304fd1b3"
      },
      "source": [
        "def build_vgg16_model(trainable_layers=-1, optimizer='adam', learning_rate=0.001, l2_strength=0.0, dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Builds and compiles a VGG16 model with customizable layers, optimizer, and regularization.\n",
        "\n",
        "    Args:\n",
        "        trainable_layers (int): Number of VGG16 layers to unfreeze for training.\n",
        "                                 -1: All layers are trainable.\n",
        "                                  0: None of the VGG16 layers are trainable.\n",
        "                                 >0: The last 'trainable_layers' are unfrozen.\n",
        "        optimizer (str): The name of the optimizer ('adam' or 'sgd').\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "        l2_strength (float): The L2 regularization strength for dense layers.\n",
        "        dropout_rate (float): The dropout rate for the dropout layer.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "    # Load the VGG16 base model\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Set the number of trainable layers in the base model\n",
        "    if trainable_layers == 0:\n",
        "        base_model.trainable = False\n",
        "    elif trainable_layers > 0:\n",
        "        for layer in base_model.layers[:-trainable_layers]:\n",
        "            layer.trainable = False\n",
        "        for layer in base_model.layers[-trainable_layers:]:\n",
        "            layer.trainable = True\n",
        "    else: # trainable_layers == -1\n",
        "         base_model.trainable = True\n",
        "\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_strength)),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Define the optimizer\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(f\"Optimizer '{optimizer}' not supported. Choose 'adam' or 'sgd'.\")\n",
        "\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30aa2afe"
      },
      "source": [
        "## Define experiment configurations\n",
        "\n",
        "Define at least seven different configurations for the experiments, varying parameters such as data augmentation, fine-tuning depth, optimizer, and regularization. I will define a list of dictionaries, each representing an experiment configuration with varying parameters like data augmentation, fine-tuning depth, optimizer, learning rate, L2 regularization, and dropout rate. This list will be used to run multiple experiments.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45717d3c",
        "outputId": "75e672de-0628-43b6-a2ac-eba7cc981064"
      },
      "source": [
        "experiment_configurations = [\n",
        "    {\n",
        "        \"experiment_name\": \"exp_1_no_aug_freeze_all_adam\",\n",
        "        \"data_augmentation\": False,\n",
        "        \"trainable_layers\": 0, # Freeze all VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "    {\n",
        "        \"experiment_name\": \"exp_2_aug_freeze_all_adam\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": 0, # Freeze all VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "    {\n",
        "        \"experiment_name\": \"exp_3_aug_unfreeze_last_adam\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": 4, # Unfreeze the last few VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.0001, # Lower learning rate for fine-tuning\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "     {\n",
        "        \"experiment_name\": \"exp_4_aug_unfreeze_all_adam\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": -1, # Unfreeze all VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.00001, # Very low learning rate for full fine-tuning\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "    {\n",
        "        \"experiment_name\": \"exp_5_aug_unfreeze_last_sgd\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": 4, # Unfreeze the last few VGG16 layers\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "    {\n",
        "        \"experiment_name\": \"exp_6_aug_unfreeze_last_adam_l2\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": 4, # Unfreeze the last few VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"l2_strength\": 0.001, # Add L2 regularization\n",
        "        \"dropout_rate\": 0.0\n",
        "    },\n",
        "    {\n",
        "        \"experiment_name\": \"exp_7_aug_unfreeze_last_adam_dropout\",\n",
        "        \"data_augmentation\": True,\n",
        "        \"trainable_layers\": 4, # Unfreeze the last few VGG16 layers\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"l2_strength\": 0.0,\n",
        "        \"dropout_rate\": 0.5 # Add dropout\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(experiment_configurations)} experiment configurations.\")\n",
        "# Display the configurations\n",
        "for config in experiment_configurations:\n",
        "    print(config)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 7 experiment configurations.\n",
            "{'experiment_name': 'exp_1_no_aug_freeze_all_adam', 'data_augmentation': False, 'trainable_layers': 0, 'optimizer': 'adam', 'learning_rate': 0.001, 'l2_strength': 0.0, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_2_aug_freeze_all_adam', 'data_augmentation': True, 'trainable_layers': 0, 'optimizer': 'adam', 'learning_rate': 0.001, 'l2_strength': 0.0, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_3_aug_unfreeze_last_adam', 'data_augmentation': True, 'trainable_layers': 4, 'optimizer': 'adam', 'learning_rate': 0.0001, 'l2_strength': 0.0, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_4_aug_unfreeze_all_adam', 'data_augmentation': True, 'trainable_layers': -1, 'optimizer': 'adam', 'learning_rate': 1e-05, 'l2_strength': 0.0, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_5_aug_unfreeze_last_sgd', 'data_augmentation': True, 'trainable_layers': 4, 'optimizer': 'sgd', 'learning_rate': 0.001, 'l2_strength': 0.0, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_6_aug_unfreeze_last_adam_l2', 'data_augmentation': True, 'trainable_layers': 4, 'optimizer': 'adam', 'learning_rate': 0.0001, 'l2_strength': 0.001, 'dropout_rate': 0.0}\n",
            "{'experiment_name': 'exp_7_aug_unfreeze_last_adam_dropout', 'data_augmentation': True, 'trainable_layers': 4, 'optimizer': 'adam', 'learning_rate': 0.0001, 'l2_strength': 0.0, 'dropout_rate': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99769173"
      },
      "source": [
        "## Run experiments\n",
        "\n",
        "Iterate through the experiment configurations, train the VGG16 model for each configuration, and save the training history and model weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f116039",
        "outputId": "e4a10fe3-2289-4601-ffdb-11ca0746605d"
      },
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "for config in experiment_configurations:\n",
        "    exp_name = config['experiment_name']\n",
        "    data_augmentation = config['data_augmentation']\n",
        "    trainable_layers = config['trainable_layers']\n",
        "    optimizer = config['optimizer']\n",
        "    learning_rate = config['learning_rate']\n",
        "    l2_strength = config['l2_strength']\n",
        "    dropout_rate = config['dropout_rate']\n",
        "\n",
        "    print(f\"\\nStarting experiment: {exp_name}\")\n",
        "\n",
        "    # Create experiment output directory\n",
        "    exp_output_dir = os.path.join(OUTPUT_DIR, exp_name)\n",
        "    os.makedirs(exp_output_dir, exist_ok=True)\n",
        "\n",
        "    # Save configuration\n",
        "    with open(os.path.join(exp_output_dir, 'config.json'), 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    # Define data generators\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    ) if data_augmentation else tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rescale=1./255\n",
        "    )\n",
        "\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rescale=1./255\n",
        "    )\n",
        "\n",
        "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rescale=1./255\n",
        "    )\n",
        "\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        os.path.join(SPLIT_ROOT, 'train'),\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        os.path.join(SPLIT_ROOT, 'val'),\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        os.path.join(SPLIT_ROOT, 'test'),\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    # Compute class weights\n",
        "    classes = np.unique(train_generator.classes)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=classes,\n",
        "        y=train_generator.classes\n",
        "    )\n",
        "    class_weight_dict = dict(zip(classes, class_weights))\n",
        "    print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_vgg16_model(\n",
        "        trainable_layers=trainable_layers,\n",
        "        optimizer=optimizer,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_strength=l2_strength,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    # Define callbacks\n",
        "    checkpoint_filepath = os.path.join(exp_output_dir, 'best_model.weights.h5')\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    )\n",
        "\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    callbacks = [model_checkpoint_callback, early_stopping_callback]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_generator,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Save training history\n",
        "    history_dict = history.history\n",
        "    with open(os.path.join(exp_output_dir, 'history.json'), 'w') as f:\n",
        "        json.dump(history_dict, f)\n",
        "\n",
        "    # Save the final model weights\n",
        "    model.save_weights(os.path.join(exp_output_dir, 'final_model.weights.h5'))\n",
        "\n",
        "    print(f\"Experiment {exp_name} finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment: exp_1_no_aug_freeze_all_adam\n",
            "Found 22046 images belonging to 2 classes.\n",
            "Found 2754 images belonging to 2 classes.\n",
            "Found 2758 images belonging to 2 classes.\n",
            "Class weights: {np.int32(0): np.float64(1.0), np.int32(1): np.float64(1.0)}\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 190ms/step - accuracy: 0.7580 - loss: 0.4939 - val_accuracy: 0.9049 - val_loss: 0.2593\n",
            "Epoch 2/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9015 - loss: 0.2588 - val_accuracy: 0.9096 - val_loss: 0.2271\n",
            "Epoch 3/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9078 - loss: 0.2396 - val_accuracy: 0.9168 - val_loss: 0.2087\n",
            "Epoch 4/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9199 - loss: 0.2099 - val_accuracy: 0.8744 - val_loss: 0.3004\n",
            "Epoch 5/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9196 - loss: 0.2083 - val_accuracy: 0.9194 - val_loss: 0.2024\n",
            "Epoch 6/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9222 - loss: 0.1989 - val_accuracy: 0.9158 - val_loss: 0.2127\n",
            "Epoch 7/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9196 - loss: 0.2033 - val_accuracy: 0.9256 - val_loss: 0.1880\n",
            "Epoch 8/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9290 - loss: 0.1948 - val_accuracy: 0.9299 - val_loss: 0.1839\n",
            "Epoch 9/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9274 - loss: 0.1931 - val_accuracy: 0.9292 - val_loss: 0.1810\n",
            "Epoch 10/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9277 - loss: 0.1890 - val_accuracy: 0.9219 - val_loss: 0.2047\n",
            "Experiment exp_1_no_aug_freeze_all_adam finished.\n",
            "\n",
            "Starting experiment: exp_2_aug_freeze_all_adam\n",
            "Found 22046 images belonging to 2 classes.\n",
            "Found 2754 images belonging to 2 classes.\n",
            "Found 2758 images belonging to 2 classes.\n",
            "Class weights: {np.int32(0): np.float64(1.0), np.int32(1): np.float64(1.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 464ms/step - accuracy: 0.7530 - loss: 0.4988 - val_accuracy: 0.8936 - val_loss: 0.2943\n",
            "Epoch 2/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 460ms/step - accuracy: 0.8908 - loss: 0.2806 - val_accuracy: 0.9016 - val_loss: 0.2593\n",
            "Epoch 3/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 464ms/step - accuracy: 0.9088 - loss: 0.2430 - val_accuracy: 0.9143 - val_loss: 0.2309\n",
            "Epoch 4/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 467ms/step - accuracy: 0.9068 - loss: 0.2348 - val_accuracy: 0.9161 - val_loss: 0.2238\n",
            "Epoch 5/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 464ms/step - accuracy: 0.9086 - loss: 0.2358 - val_accuracy: 0.9147 - val_loss: 0.2230\n",
            "Epoch 6/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 464ms/step - accuracy: 0.9148 - loss: 0.2184 - val_accuracy: 0.9147 - val_loss: 0.2170\n",
            "Epoch 7/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 476ms/step - accuracy: 0.9184 - loss: 0.2146 - val_accuracy: 0.9081 - val_loss: 0.2346\n",
            "Epoch 8/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 465ms/step - accuracy: 0.9205 - loss: 0.2108 - val_accuracy: 0.8932 - val_loss: 0.2780\n",
            "Epoch 9/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 465ms/step - accuracy: 0.9124 - loss: 0.2259 - val_accuracy: 0.9110 - val_loss: 0.2283\n",
            "Epoch 10/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 461ms/step - accuracy: 0.9202 - loss: 0.2113 - val_accuracy: 0.9245 - val_loss: 0.2033\n",
            "Experiment exp_2_aug_freeze_all_adam finished.\n",
            "\n",
            "Starting experiment: exp_3_aug_unfreeze_last_adam\n",
            "Found 22046 images belonging to 2 classes.\n",
            "Found 2754 images belonging to 2 classes.\n",
            "Found 2758 images belonging to 2 classes.\n",
            "Class weights: {np.int32(0): np.float64(1.0), np.int32(1): np.float64(1.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 476ms/step - accuracy: 0.8882 - loss: 0.2679 - val_accuracy: 0.9535 - val_loss: 0.1389\n",
            "Epoch 2/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 471ms/step - accuracy: 0.9544 - loss: 0.1356 - val_accuracy: 0.9539 - val_loss: 0.1433\n",
            "Epoch 3/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 480ms/step - accuracy: 0.9551 - loss: 0.1285 - val_accuracy: 0.9579 - val_loss: 0.1263\n",
            "Epoch 4/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 468ms/step - accuracy: 0.9588 - loss: 0.1239 - val_accuracy: 0.9604 - val_loss: 0.1208\n",
            "Epoch 5/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 471ms/step - accuracy: 0.9549 - loss: 0.1237 - val_accuracy: 0.9608 - val_loss: 0.1212\n",
            "Epoch 6/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 463ms/step - accuracy: 0.9597 - loss: 0.1136 - val_accuracy: 0.9582 - val_loss: 0.1219\n",
            "Epoch 7/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 468ms/step - accuracy: 0.9613 - loss: 0.1111 - val_accuracy: 0.9601 - val_loss: 0.1108\n",
            "Epoch 8/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 471ms/step - accuracy: 0.9648 - loss: 0.1020 - val_accuracy: 0.9615 - val_loss: 0.1071\n",
            "Epoch 9/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 475ms/step - accuracy: 0.9644 - loss: 0.1039 - val_accuracy: 0.9586 - val_loss: 0.1141\n",
            "Epoch 10/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 476ms/step - accuracy: 0.9657 - loss: 0.0990 - val_accuracy: 0.9615 - val_loss: 0.1147\n",
            "Experiment exp_3_aug_unfreeze_last_adam finished.\n",
            "\n",
            "Starting experiment: exp_4_aug_unfreeze_all_adam\n",
            "Found 22046 images belonging to 2 classes.\n",
            "Found 2754 images belonging to 2 classes.\n",
            "Found 2758 images belonging to 2 classes.\n",
            "Class weights: {np.int32(0): np.float64(1.0), np.int32(1): np.float64(1.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 647ms/step - accuracy: 0.8679 - loss: 0.2894 - val_accuracy: 0.9601 - val_loss: 0.1207\n",
            "Epoch 2/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 603ms/step - accuracy: 0.9607 - loss: 0.1167 - val_accuracy: 0.9659 - val_loss: 0.1030\n",
            "Epoch 3/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 608ms/step - accuracy: 0.9640 - loss: 0.1054 - val_accuracy: 0.9637 - val_loss: 0.1021\n",
            "Epoch 4/10\n",
            "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 609ms/step - accuracy: 0.9645 - loss: 0.1055 - val_accuracy: 0.9662 - val_loss: 0.0956\n",
            "Epoch 5/10\n",
            "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4:10\u001b[0m 587ms/step - accuracy: 0.9653 - loss: 0.0976"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OHUcNb15U2vT",
        "WQPM3U9XU2vr"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}